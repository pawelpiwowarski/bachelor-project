{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> This part of code corresponds tho the research methods and the results of my thesis </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange\"> The first step is to import all libraries</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "import wandb\n",
    "import platform\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from functools import partial\n",
    "from transformers import PreTrainedModel\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import PretrainedConfig\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:orange\"> Next we introduce a set of global configs which we'll be used through the search </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cm/2c5qsgtd2z9_c__1cmj825dm0000gn/T/ipykernel_23569/1062750872.py:15: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  has_mps = getattr(torch, 'has_mps', False)\n",
      "/var/folders/cm/2c5qsgtd2z9_c__1cmj825dm0000gn/T/ipykernel_23569/1062750872.py:16: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = \"mps\" if getattr(torch, 'has_mps', False) \\\n",
      "/var/folders/cm/2c5qsgtd2z9_c__1cmj825dm0000gn/T/ipykernel_23569/1062750872.py:18: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = \"mps\" if getattr(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-14.4.1-arm64-arm-64bit\n",
      "PyTorch Version: 2.3.0\n",
      "Python 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:26:08) [Clang 14.0.6 ]\n",
      "GPU is NOT AVAILABLE\n",
      "MPS (Apple Metal) is AVAILABLE\n",
      "tensor([2.0147, 4.1631, 3.7958], device='mps:0')\n",
      "['Bacterial', 'Normal', 'Viral']\n"
     ]
    }
   ],
   "source": [
    "image_size = 224 # since we want to compare to pretrained models like ViT\n",
    "batch_size = 128 # our GPU can handle it\n",
    "num_classes = 3 # since we are doing the 3-class classification on pneumonia dataset\n",
    "epochs = 5 # since 5 epochs of pretraining per model is enough\n",
    "channels = 3 # since we are using RGB images\n",
    "seed = 1234 # for reproducibility\n",
    "random_generator = np.random.default_rng(seed)\n",
    "folder_for_best_architectures = './best_architectures/'\n",
    "dataset_path = \"pawlo2013/chest_xray\" # this is the dataset we are using it contains the exact images as the Kermany et al. dataset.\n",
    "vector_of_choices = [2, 5, 2, 3, 4, 2] # this can be done a bit more elegantly, basically this encodes all of the possible architectural choices for the model\n",
    "use_wandb = True # since we want to log the results\n",
    "population_size = 5 # number of models in the population can be higher but for the sake of GPU time we are keeping it low.\n",
    "\n",
    "# getting the device\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch, 'has_mps', False)\n",
    "device = \"mps\" if getattr(torch, 'has_mps', False) \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"mps\" if getattr(\n",
    "    torch, 'has_mps', False) else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Python {sys.version}\")\n",
    "\n",
    "\n",
    "print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# function to calculate class weights as mention in the paper\n",
    "def calculate_class_weights(dataset_path, device):\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    \n",
    "    # Assuming the label column is named 'label'\n",
    "    train_labels = dataset['train']['label']\n",
    "    \n",
    "    # Count the number of samples in each class\n",
    "    label_counts = Counter(train_labels)\n",
    "    \n",
    "    # Get total number of samples\n",
    "    total_samples = len(train_labels)\n",
    "    \n",
    "    # Calculate class weights: inverse of class frequency\n",
    "    class_weights = {label: total_samples / count for label, count in label_counts.items()}\n",
    "    \n",
    "    # Convert class weights to tensor and move to the specified device\n",
    "    class_weights_tensor = torch.tensor(\n",
    "        [class_weights[label] for label in range(len(class_weights))],\n",
    "        dtype=torch.float\n",
    "    ).to(device)\n",
    "    \n",
    "    return class_weights_tensor\n",
    "\n",
    "\n",
    "# get the class weights that will be used in the loss function\n",
    "class_weights = calculate_class_weights(dataset_path, device)\n",
    "\n",
    "\n",
    "# here we are just loading the dataset, to get the class names. \n",
    "dataset = load_dataset(dataset_path)\n",
    "train_dataset = dataset['train']\n",
    "class_names = train_dataset.features['label'].names\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange\"> Reusable code, some helper functions</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually the CustomDataset is not really needed it can be dene by using a collate function in the Trainer class.\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, device, transform=None, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        \n",
    "        if shuffle:\n",
    "            self.dataset = self.dataset.shuffle(seed)\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"]\n",
    "        label = item[\"label\"]\n",
    "        if self.transform:\n",
    "            image = self.transform(image, self.data_augmentation)\n",
    "        return {\"input_ids\": image, \"labels\": label}\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "# the initial transforms that will be used for the images as mentioned in the paper scales to [-1,1]\n",
    "initial_transforms = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((image_size, image_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Lambda(lambda t: (t * 2) - 1),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "# callback function that gets called on every step for on the fly data normalization\n",
    "def perform_transforms(image):\n",
    "    image = initial_transforms(image.convert(\"RGB\"))\n",
    "    return image\n",
    "\n",
    "\n",
    "# funtion for initing the population of the genetic algorithm\n",
    "def init_population(population_size, vector_of_choices):\n",
    "\n",
    "    population = []\n",
    "\n",
    "    for _ in range(population_size):\n",
    "        individual = []\n",
    "        for length in vector_of_choices:\n",
    "\n",
    "            value = random.randint(0, length - 1)\n",
    "            individual.append(value)\n",
    "\n",
    "        population.append(individual)  \n",
    "\n",
    "    return np.asarray(population)          \n",
    "\n",
    "\n",
    "# the decode function mentioned in the paper, again this can be done more elegantly. \n",
    "def decode_vector(x):\n",
    "     return [\n",
    "            x[0] % 2,\n",
    "            x[1] % 4,\n",
    "            x[2] % 2,\n",
    "            x[3] % 3,\n",
    "            x[4] % 4,\n",
    "            x[5] % 2,\n",
    "   \n",
    "     ]\n",
    "\n",
    "# an important function that is used to remove duplicates from the population\n",
    "def remove_duplicates_order_matters(lst):\n",
    "    seen = set()\n",
    "    result = []\n",
    "\n",
    "    for sublist in lst:\n",
    "        sublist_tuple = tuple(sublist)\n",
    "        if sublist_tuple not in seen:\n",
    "            result.append(sublist)\n",
    "            seen.add(sublist_tuple)\n",
    "\n",
    "    return  np.asarray(result)\n",
    "# helper function to check if a list is in a nested list\n",
    "def is_list_not_in_nested_list(list1, nested_list):\n",
    "    # Convert the nested list elements to tuples for comparison\n",
    "    nested_list_tuples = [tuple(lst) for lst in nested_list]\n",
    "    list1_tuple = tuple(list1)\n",
    "\n",
    "    return list1_tuple not in nested_list_tuples\n",
    "\n",
    "# metrics that will be used for the evaluation of the model important to get the per class accuracies\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    # Overall accuracy\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    # Precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=0)     \n",
    "    # Confusion matrix to calculate per-class accuracy\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    # Prepare the per-class accuracy in a dictionary\n",
    "    per_class_accuracy = {f\"accuracy_class_{class_names[i]}\": acc for i, acc in enumerate(per_class_acc)}\n",
    "    # Combine all metrics into one dictionary\n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "    metrics.update(per_class_accuracy)\n",
    "    # log the confusion matrix to W&B\n",
    "    wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                y_true=labels,\n",
    "                                                                preds=preds,\n",
    "                                                                class_names=class_names)})   \n",
    "    return metrics\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Custom config class for the ConvNext model, can be useful when exporting the mdoel to the Hugging Face model hub.\n",
    "class CustomConvNextConfig(PretrainedConfig):\n",
    "    model_type = \"custom_convnext\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_channels: int = 3,\n",
    "                 num_classes: int = 3,\n",
    "                 device: str = 'cpu',\n",
    "                 convnext_mult: int = 2,\n",
    "                 layer_norm: bool = False,  # Fixed Bool to bool\n",
    "                 dropout: float = 0.0,\n",
    "                 pooling_choice: str = \"MaxPool2d\",\n",
    "                 activation_choice: str = \"ReLU\",\n",
    "                 use_residual: bool = False,\n",
    "                 **kwargs  # Add a comma before **kwargs\n",
    "                 ):\n",
    "\n",
    "        # Initialize class attributes\n",
    "        self.input_channels = input_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.convnext_mult = convnext_mult\n",
    "        self.layer_norm = layer_norm\n",
    "        self.dropout = dropout\n",
    "        self.pooling_choice = pooling_choice\n",
    "        self.activation_choice = activation_choice\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "        # Call the super class's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange\"> Basic CNN Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this function is used to calculate the number of halving steps that will be used in the model\n",
    "# since the last we want the last feature map to be Cx1x1 where C is the number of resulting channels\n",
    "# using something like a global average pooling layer results in too many parameters\n",
    "def calculate_halving_steps(input_size):\n",
    "    # Ensure the input size is greater than 0\n",
    "    if input_size <= 0:\n",
    "        raise ValueError(\"The input size must be a positive integer\")\n",
    "    # Initialize the halving step counter\n",
    "    halving_steps = 0\n",
    "    # Keep halving the size until it reaches 1\n",
    "    while input_size > 1:\n",
    "        input_size = input_size // 2\n",
    "        halving_steps += 1\n",
    "    return halving_steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the main function that will be used to train the model\n",
    "class PretrainedWraper(PreTrainedModel):\n",
    "\n",
    "    config_class = CustomConvNextConfig\n",
    "\n",
    "\n",
    "    def __init__(self, config, class_weights: torch.Tensor = torch.tensor([1.0,1.0,1.0])):\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.class_weights = class_weights.to(config.device)\n",
    "\n",
    "        self.model = CNN(\n",
    "            input_size=config.image_size,\n",
    "            device=config.device,\n",
    "            num_classes=config.num_classes,\n",
    "            pooling_choice=config.pooling_choice,\n",
    "            activation_choice=config.activation_choice,\n",
    "            convnext_mult=config.convnext_mult,\n",
    "            layer_norm=config.layer_norm,\n",
    "            dropout=config.dropout,\n",
    "            use_residual=config.use_residual,\n",
    "        ).to(config.device)\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        logits = self.model(input_ids)\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.NLLLoss(weight=self.class_weights)(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "\n",
    "# the main 'dynamic' CNN model that will be used in the genetic algorithm\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, channels=3, pooling_choice=None, activation_choice=None, convnext_mult=None, layer_norm = None, dropout=None, use_residual=None ):\n",
    "        super(CNN, self).__init__()\n",
    "        # check how many halving steps we can do\n",
    "        self.halving_steps = calculate_halving_steps(input_size)\n",
    "        # set the number of classes\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        self.conv_layers = nn.ModuleList([])\n",
    "        self.input_channels = [channels * (2**i) for i in range(self.halving_steps)]\n",
    "        self.output_channels = [\n",
    "            channels * (2 ** (i + 1)) for i in range(self.halving_steps)\n",
    "        ]\n",
    "        self.combinations = list(zip(self.input_channels, self.output_channels))\n",
    "        self.pooling_choice = pooling_choice\n",
    "       \n",
    "        block_klass = partial(ConvNextBlock, mult=convnext_mult, activation_choice=activation_choice, norm=layer_norm, use_residual=use_residual)\n",
    "\n",
    "        for i, (input_channels, output_channels) in enumerate(self.combinations):\n",
    "            is_last = i == self.halving_steps - 1\n",
    "            self.conv_layers.append(\n",
    "                nn.Sequential(\n",
    "                    block_klass(input_channels, output_channels),\n",
    "                    self.get_pooling_layer()(kernel_size=2, stride=2),\n",
    "                )\n",
    "            )\n",
    "            if is_last:\n",
    "                self.conv_layers.append(\n",
    "                    nn.Sequential(\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(output_channels, output_channels // 2),\n",
    "                        nn.Dropout(dropout),\n",
    "                        nn.Linear(output_channels // 2, num_classes),\n",
    "                        nn.LogSoftmax(dim=1),\n",
    "                    )\n",
    "                )    \n",
    "    def get_pooling_layer(self):\n",
    "        return getattr(nn, self.pooling_choice)\n",
    "    def forward(self, input_ids):\n",
    "        x = input_ids\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/2201.03545\"\"\"\n",
    "\n",
    "    def __init__(self, dim, dim_out, *, mult=2, norm=True, activation_choice, use_residual=False):\n",
    "        super().__init__()\n",
    "        self.use_residual = use_residual\n",
    "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
    "        self.activation_choice = activation_choice\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # equivalent to LayerNorm \n",
    "            nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
    "            nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
    "            self.get_activation_layer(),\n",
    "            # equivalent to LayerNorm \n",
    "            nn.GroupNorm(1, dim_out * mult) if norm else nn.Identity(),\n",
    "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
    "        )\n",
    "        if use_residual:\n",
    "            self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def get_activation_layer(self):\n",
    "        return getattr(nn, self.activation_choice)()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.ds_conv(x)\n",
    "        h = self.net(h)\n",
    "        if self.use_residual:\n",
    "            return h + self.res_conv(x)\n",
    "        return h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for the architecture that faciliteates teh genetic algorithm, sort of a wrapper around the model.\n",
    "\n",
    "class MetaNeuralFramework():\n",
    "    def __init__(self, image_size, num_classes, epochs, batch_size, population, class_weigths, device , use_wandb, dataset_path, f1 = False ):\n",
    "\n",
    "\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.population = population\n",
    "        self.class_weigths = class_weigths\n",
    "        self.device = device\n",
    "        self.use_wandb = use_wandb\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "\n",
    "        # IMPORTANT!!\n",
    "        # This encodes possible choices for the model architecture\n",
    "        # This should correspond to the choice vector that is globally defined\n",
    "        # Again extremely hacky, but it works for now, can be done more elegantly.\n",
    "        self.layer_norm = [True,False]\n",
    "        self.activations = [\"ReLU\", \"GELU\", \"Tanh\", \"Softplus\", \"Sigmoid\"]\n",
    "        self.pooling_layer_choices = [\"MaxPool2d\", \"AvgPool2d\"]\n",
    "        self.dropout = [0, 0.33, 0.66]\n",
    "        self.learning_rate = [0.00001, 0.0001, 0.001, 0.01]\n",
    "        self.use_residual = [True, False]\n",
    "\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.f1 = f1\n",
    "\n",
    "\n",
    "\n",
    "        self.dataset = load_dataset(self.dataset_path)\n",
    "        self.class_names =  self.dataset[\"train\"].features[\"label\"].names\n",
    "\n",
    "        self.train_loader = CustomDataset( self.dataset[\"train\"], transform=perform_transforms, shuffle=True, device=device)\n",
    "        self.val_loader = CustomDataset( self.dataset[\"validation\"], transform=perform_transforms, shuffle=False, device=device)\n",
    "        self.test_loader = CustomDataset( self.dataset[\"test\"], transform=perform_transforms, shuffle=False, device=device)\n",
    "\n",
    "  \n",
    "       \n",
    "\n",
    "\n",
    "    def decode(self):\n",
    "\n",
    "        self.decoded_vector = decode_vector(self.vector)\n",
    "\n",
    "        return self.decoded_vector\n",
    "\n",
    "    # Function for preparing the run\n",
    "    # It gets as input a vector that encodes the model and based on this vector it prepares the model for training.\n",
    "    def prepare_run(self, vector):\n",
    "\n",
    "        decoded_vector = decode_vector(vector)\n",
    "\n",
    "        layer_norm = self.layer_norm [decoded_vector[0]]\n",
    "        activation_choice = self.activations[decoded_vector[1]]\n",
    "        pooling_choice = self.pooling_layer_choices[decoded_vector[2]]\n",
    "        convnext_mult = 1\n",
    "        dropout = self.dropout[decoded_vector[3]]\n",
    "        learning_rate = self.learning_rate[decoded_vector[4]]\n",
    "        use_residual = self.use_residual[decoded_vector[5]]\n",
    "\n",
    "        data_augmentation = self.data_augmentation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Layer Norm: {layer_norm}\")\n",
    "        print(f\"Activation: {activation_choice}\")\n",
    "        print(f\"ConvNext Multiplier: {convnext_mult}\")\n",
    "        print(f\"Pooling Layer: {pooling_choice}\")\n",
    "        print(f\"Dropout: {dropout}\")\n",
    "        print(\"epochs\", self.epochs)\n",
    "        print(\"batch_size\", self.batch_size)\n",
    "        print(\"class_weigths\", self.class_weigths)\n",
    "        print(\"device\", self.device)\n",
    "        print('data_augmentation', data_augmentation)\n",
    "        print(\"use_wandb\", self.use_wandb)\n",
    "        print(\"learning_rate\", learning_rate)\n",
    "        print(\"use_residual\", use_residual)\n",
    "\n",
    "\n",
    "        config = CustomConvNextConfig(\n",
    "            input_channels=channels,\n",
    "            num_classes=num_classes,\n",
    "            device=self.device,\n",
    "            convnext_mult=convnext_mult,\n",
    "            layer_norm=layer_norm,\n",
    "            dropout=dropout,\n",
    "            pooling_choice=pooling_choice,\n",
    "            activation_choice=activation_choice,\n",
    "            image_size=image_size,\n",
    "            use_residual=use_residual,\n",
    "        )\n",
    "\n",
    "        model = PretrainedWraper(config, class_weights=self.class_weigths)\n",
    "\n",
    "\n",
    "        self.num_of_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        wandb.init(project=\"neural_evolution_variable_learning_variable_residual_is_long\", \n",
    "    \n",
    "                    name=str(decoded_vector), config=\n",
    "                   \n",
    "                   {\n",
    "                       \n",
    "                          \"layer_norm\": layer_norm, \n",
    "                            \"activation_choice\": activation_choice,\n",
    "                            \"convnext_mult\": convnext_mult,\n",
    "                            \"pooling_choice\": pooling_choice,\n",
    "                            \"dropout\": dropout,\n",
    "                            \"epochs\": self.epochs,\n",
    "                            \"batch_size\": self.batch_size,\n",
    "                            \"class_weigths\": self.class_weigths,\n",
    "                            \"device\": self.device,\n",
    "                            \"data_augmentation\": data_augmentation,\n",
    "                            \"num_of_params\": self.num_of_params,\n",
    "                            \"learning_rate\": learning_rate,\n",
    "                            \"use_residual\": use_residual, \n",
    "\n",
    "\n",
    "                            \n",
    "\n",
    "                   }\n",
    "                   \n",
    "                     )\n",
    "        \n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            num_train_epochs=self.epochs,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            logging_dir=\"./logs\",\n",
    "            logging_steps=10,\n",
    "            weight_decay=0.01,\n",
    "            save_strategy=\"epoch\",\n",
    "            report_to=\"wandb\" if self.use_wandb else None,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            learning_rate=learning_rate,\n",
    "            \n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        # Trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.train_loader,\n",
    "            eval_dataset=self.val_loader,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "\n",
    "    def evaluate(self, vector, generation):\n",
    "        \n",
    "        self.vector = vector\n",
    "        self.prepare_run(self.vector)\n",
    "        self.trainer.train()\n",
    "        self.results = self.trainer.evaluate(self.test_loader)\n",
    "\n",
    "        if self.f1:\n",
    "\n",
    "            fitness = self.results['eval_f1'] \n",
    "            wandb.log({\"fitness\":fitness, \"generation\": generation})\n",
    "            wandb.finish()\n",
    "            return fitness\n",
    "        \n",
    "        else:\n",
    "\n",
    "            fitness = self.results['eval_accuracy_class_Viral'] + self.results['eval_accuracy_class_Bacterial'] \n",
    "            wandb.log({\"fitness\": fitness,  \"generation\": generation})\n",
    "            wandb.finish()\n",
    "            return self.results['eval_accuracy_class_Viral'] + self.results['eval_accuracy_class_Bacterial'] \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The code for the evolutionary algorithm, majority of it was taken from the paper https://arxiv.org/abs/2002.02869\n",
    "class MetaEvolutionaryAlgoritm():\n",
    "    def __init__(self, scaling_factor = 2, cross_over_probability=0.9, framework = None, number_of_survivors = 5):\n",
    "        self.number_of_survivors = number_of_survivors\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.cross_over_probability = cross_over_probability\n",
    "        self.framework = framework\n",
    "        # helper functions\n",
    "        self.remove_duplicates = remove_duplicates_order_matters\n",
    "        self.decode = decode_vector\n",
    "        self.check = is_list_not_in_nested_list\n",
    "\n",
    "    def differential_mutation(self):\n",
    "        # implemented after the paper https://arxiv.org/abs/2002.02869\n",
    "        self.mutants = np.ones((self.x.shape[0] * 3, self.x.shape[1]), dtype=int)\n",
    "        for i in range(0,self.mutants.shape[0],3):\n",
    "            # sample 3 random indexes from the population\n",
    "            random_index = np.random.choice(self.x.shape[0], 3, replace=False)\n",
    "            # sample 3 random vectors from the population\n",
    "            random_vectors = self.x[random_index]\n",
    "            # perform the mutation on the 3 random vectors from the population\n",
    "            self.mutants [i] = random_vectors[0] + self.scaling_factor * (random_vectors[1] + random_vectors[2])\n",
    "            self.mutants [i+1] = random_vectors[1] + self.scaling_factor * (random_vectors[2] + self.mutants[i])\n",
    "            self.mutants [i+2] = random_vectors[2] + self.scaling_factor * (self.mutants [i] + self.mutants [i+1])\n",
    "\n",
    "    def uniform_cross_over(self):\n",
    "        # the population after the crossover is initialized with zeros\n",
    "        self.crossover_population = np.zeros((self.mutants.shape[0], self.mutants.shape[1]), dtype=int)\n",
    "        # the mask is a vector of 0s and 1s with the same size as the number of genes and the probability \n",
    "        # of 0s is the cross over probability being 0.9 after the paper\n",
    "        mask = np.random.binomial(n=1, p=self.cross_over_probability, size=(self.x.shape[1])) \n",
    "        # similar to the mutation we are performing the crossover on 3 vectors at a time\n",
    "        for i in range(0, self.mutants.shape[0], 3):\n",
    "            self.crossover_population[i] = mask * self.mutants[i] + (1 - mask) * self.x[i//3]\n",
    "            self.crossover_population[i +1 ] = mask * self.mutants[i + 1] + self.x[i//3] * (1 - mask)\n",
    "            self.crossover_population[i + 2] = mask * self.mutants[i + 2] + self.x[i//3] * (1 - mask)\n",
    "\n",
    "    def remove_duplicates_order_matters_and_decode(self):\n",
    "        # remove duplicates and decode the vectors this is a very important step \n",
    "        # it saves a lot of computation \n",
    "        # and it keeps a uniformity of the population\n",
    "        self.crossover_population = np.asarray([self.decode(member) for member in self.crossover_population])\n",
    "        self.crossover_population = self.remove_duplicates(self.crossover_population)           \n",
    "\n",
    "    def evaluate(self):\n",
    "        f_evaluated = []\n",
    "        for x in (self.crossover_population):\n",
    "            # check if the vector has been already evaluate        \n",
    "            # saves on computation\n",
    "            if (self.check(x, self.x) and self.check(x, self.visited)):\n",
    "                f_evaluated.append(self.framework.evaluate(x,self.current_generation))\n",
    "                self.visited.append(x)\n",
    "            else:\n",
    "                # We add -1 since the recall can never be negative\n",
    "                # If already evaluated we add -1 to the list\n",
    "                f_evaluated.append(-1)\n",
    "        return f_evaluated\n",
    "    \n",
    "    def evolution(self, x, f, visited, current_generation):\n",
    "        self.x = x\n",
    "        self.f = f\n",
    "        self.visited = visited\n",
    "        self.current_generation = current_generation\n",
    "        # print the best member of the population\n",
    "        print('Best member of the population: ', self.x[0])\n",
    "        self.differential_mutation()\n",
    "        self.uniform_cross_over()\n",
    "        self.remove_duplicates_order_matters_and_decode()\n",
    "        f_evaluated = self.evaluate()\n",
    "        # select 5 best member of both the crossover population and the original population\n",
    "        self.x = np.concatenate((self.x, self.crossover_population))\n",
    "        self.f = np.concatenate((self.f, f_evaluated))\n",
    "        # sort the population based on the fitness value from highest to lowest\n",
    "        sorted_index = np.argsort(self.f)[::-1]\n",
    "        self.x = self.x[sorted_index]\n",
    "        self.f = self.f[sorted_index]\n",
    "        # select the best 5 members of the population\n",
    "        self.x = self.x[:self.number_of_survivors]\n",
    "        self.f = self.f[:self.number_of_survivors]\n",
    "        # decode\n",
    "        self.x = np.asarray([self.decode(member) for member in self.x])\n",
    "        print('Current population: ', self.x)\n",
    "        print('Current fitness: ', self.f)\n",
    "        if self.f[0] < self.f[-1]:\n",
    "\n",
    "            raise ValueError('The algorithm did not converge')\n",
    "\n",
    "        return self.x, self.f, self.visited\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing the Nural Evolution Framework\")\n",
    "print(\"=====================================================================================================\")\n",
    "\n",
    "\n",
    "# if the folder does not exist create it\n",
    "if not os.path.exists(folder_for_best_architectures):\n",
    "    os.makedirs(folder_for_best_architectures)\n",
    "\n",
    "\n",
    "x = init_population(population_size, vector_of_choices)\n",
    "\n",
    "framework = MetaNeuralFramework(image_size, num_classes, epochs, batch_size, x, class_weights, device , use_wandb, dataset_path)\n",
    "meta = MetaEvolutionaryAlgoritm(framework=framework, number_of_survivors=population_size)\n",
    "\n",
    "# initialize the population with random numbers\n",
    "\n",
    "# initialize the fitness with minus values \n",
    "f =  np.ones(population_size) * -1\n",
    "# number of generations\n",
    "generations = 10\n",
    "# keep track of the visited vectors\n",
    "# saves on computation! we dont have to evaluate the same vector twice\n",
    "visited = []\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(generations)):\n",
    "    tqdm.write(\"################Generation################: \" + str(i))\n",
    "    print(\"=====================================================================================================\")\n",
    "    print(\"Current Population: \", x)\n",
    "    print(\"Current Fitness: \", f)\n",
    "\n",
    "    x,f,visited  = meta.evolution(x=x, f=f, visited=visited, current_generation=i)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:teal'> After the training get the best solutions to a txt file</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=====================================================================================================\")\n",
    "print(\"Best solutions found are:\")\n",
    "print(x)\n",
    "print(\"With fitness values:\")\n",
    "print(f)\n",
    "print(len(visited))\n",
    "\n",
    "# save the best solutions into a txt file\n",
    "\n",
    "with open(folder_for_best_architectures  + 'best_solutions.txt', 'w') as filehandle:\n",
    "\n",
    "    filehandle.write('Best solutions found are:\\n')\n",
    "    for listitem in x:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "    filehandle.write('With fitness values:\\n')\n",
    "    for listitem in f:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(filename):\n",
    "    solutions = []\n",
    "    fitnesses = []\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        # Find the start of the solutions\n",
    "        sol_start_index = lines.index(\"Best solutions found are:\\n\") + 1\n",
    "        fit_start_index = lines.index(\"With fitness values:\\n\") + 1\n",
    "\n",
    "        # Read solutions\n",
    "        for i in range(sol_start_index, fit_start_index - 1):\n",
    "            line = lines[i].strip()\n",
    "            if line:\n",
    "                solution = [int(x) for x in line.strip('[]').split(',')]\n",
    "                solutions.append(solution)\n",
    "\n",
    "        # Read fitness values\n",
    "        for i in range(fit_start_index, len(lines)):\n",
    "            line = lines[i].strip()\n",
    "            if line:\n",
    "                fitness = float(line)\n",
    "                fitnesses.append(fitness)\n",
    "\n",
    "    return solutions, fitnesses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x, f = parse_file(folder_for_best_architectures + 'best_solutions.txt')\n",
    "\n",
    "print(\"Solutions:\", x)\n",
    "print(\"Fitnesses:\", f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:orange'> Full (grid search) training of the final (best) population\n",
    " </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "final_epochs = 50\n",
    "early_stopping = 5\n",
    "data_augmentation = False\n",
    "convnext_multiplier_choices = [1, 2, 3]\n",
    "number_of_repeats = 10\n",
    "test_accuracies_array = np.ones((len(x), len(convnext_multiplier_choices))) \n",
    "final_population = x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "framework = MetaNeuralFramework(image_size, num_classes, final_epochs, batch_size, final_population, class_weights, device , use_wandb, dataset_path, data_augmentation=data_augmentation )\n",
    "for i, individual in enumerate(final_population):\n",
    "    for j, convnext_multiplier in enumerate(convnext_multiplier_choices):\n",
    "        avg = 0\n",
    "        for k in range(number_of_repeats):\n",
    "\n",
    "            decoded_vector = decode_vector(individual)\n",
    "            name = str(decoded_vector) + \"_mult_\" + str(convnext_multiplier) + \"_run_nr_\" + str(k)\n",
    "\n",
    "\n",
    "\n",
    "            layer_norm = framework.layer_norm[decoded_vector[0]]\n",
    "            activation_choice = framework.activations[decoded_vector[1]]\n",
    "            convnext_mult = convnext_multiplier\n",
    "            pooling_choice = framework.pooling_layer_choices[decoded_vector[2]]\n",
    "            dropout = framework.dropout[decoded_vector[3]]\n",
    "            learning_rate = framework.learning_rate[decoded_vector[4]]\n",
    "            use_residual = framework.use_residual[decoded_vector[5]]\n",
    "            config = CustomConvNextConfig(\n",
    "                input_channels=channels,\n",
    "                num_classes=num_classes,\n",
    "                device=device,\n",
    "                convnext_mult=convnext_mult,\n",
    "                layer_norm=layer_norm,\n",
    "                dropout=dropout,\n",
    "                pooling_choice=pooling_choice,\n",
    "                activation_choice=activation_choice,\n",
    "                image_size=image_size,\n",
    "                use_residual=use_residual\n",
    "            )\n",
    "            model_to_evaluate = PretrainedWraper(config, class_weights=class_weights)\n",
    "            num_of_params = sum(p.numel() for p in model_to_evaluate.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "            print(f\"Layer Norm: {layer_norm}\")\n",
    "            print(f\"Activation: {activation_choice}\")\n",
    "            print(f\"ConvNext Multiplier: {convnext_mult}\")\n",
    "            print(f\"Pooling Layer: {pooling_choice}\")\n",
    "            print(f\"Dropout: {dropout}\")\n",
    "            print(\"epochs\", final_epochs)\n",
    "            print(\"batch_size\", batch_size)\n",
    "            print(\"class_weigths\", class_weights)\n",
    "            print(\"device\", device)\n",
    "            print('data_augmentation', data_augmentation)\n",
    "            print('num_of_params', num_of_params)\n",
    "            print(\"learning_rate\", learning_rate)\n",
    "            print(\"use_residual\", use_residual)\n",
    "\n",
    "\n",
    "            wandb.init(project=\"neural_evolution_final_best_evaluation_stopping_on_val_loss_10_runs_each_no_data_augmentation\",\n",
    "                        name=str(decoded_vector) +\"_mult_\"+ str(convnext_mult) + \"_run_nr_\" + str(k), config=\n",
    "                        {\n",
    "                            \"layer_norm\": layer_norm,\n",
    "                            \"activation_choice\": activation_choice,\n",
    "                            \"convnext_mult\": convnext_mult,\n",
    "                            \"pooling_choice\": pooling_choice,\n",
    "                            \"dropout\": dropout,\n",
    "                            \"num_of_params\": num_of_params,\n",
    "                            \"epochs\": final_epochs,\n",
    "                            \"batch_size\": batch_size,\n",
    "                            \"class_weigths\": class_weights,\n",
    "                            \"device\": device,\n",
    "                            \"data_augmentation\": data_augmentation, \n",
    "                            \"learning_rate\": learning_rate,\n",
    "                            \"use_residual\": use_residual,\n",
    "            \n",
    "                        }\n",
    "                        )\n",
    "    \n",
    "            training_args = TrainingArguments(\n",
    "\n",
    "                output_dir=\"./results\",\n",
    "                num_train_epochs=final_epochs,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=batch_size,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                logging_dir=\"./logs\",\n",
    "                logging_steps=10,\n",
    "                weight_decay=0.01,\n",
    "                save_strategy=\"epoch\",\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_loss\",\n",
    "                report_to=\"wandb\" if use_wandb else None,\n",
    "                overwrite_output_dir=True,\n",
    "                lr_scheduler_type=\"cosine\",\n",
    "                learning_rate=learning_rate,\n",
    "                use_mps_device=True if device == \"mps\" else False,\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=model_to_evaluate,\n",
    "                args=training_args,\n",
    "                train_dataset=framework.train_loader,\n",
    "                eval_dataset=framework.val_loader,\n",
    "                compute_metrics=compute_metrics,\n",
    "                callbacks=[\n",
    "\n",
    "        EarlyStoppingCallback(early_stopping_patience=early_stopping)\n",
    "\n",
    "                ],\n",
    "            )   \n",
    "\n",
    "            print(trainer.model.device)\n",
    "\n",
    "\n",
    "\n",
    "            trainer.train()\n",
    "\n",
    "            results = trainer.evaluate(framework.test_loader, metric_key_prefix='test')\n",
    "\n",
    "            avg += results['test_accuracy']\n",
    "        \n",
    "            wandb.finish()\n",
    "        test_accuracies_array[i][j] = avg / number_of_repeats\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy, Standard Deviation, and Number of Parameters for each Architecture-Multiplier combination:\n",
      "Best Architecture-Multiplier combination: [True, 'ReLU', 'MaxPool2d', 0.33, 0.0001, True]_mult_2\n",
      "Mean Test Accuracy: 0.7814\n",
      "Best run key: [True, 'ReLU', 'MaxPool2d', 0.33, 0.0001, True]_mult_2\n",
      "Highest mean test accuracy: 0.7813993174061433\n",
      "Test accuracies that make up the best mean:\n",
      "[0.7662116040955631, 0.7969283276450512, 0.757679180887372, 0.7986348122866894, 0.7662116040955631, 0.7986348122866894, 0.7662116040955631, 0.7986348122866894, 0.7662116040955631, 0.7986348122866894]\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Initialize the API\n",
    "api = wandb.Api()\n",
    "final_epochs = 50\n",
    "early_stopping = 5\n",
    "data_augmentation = False\n",
    "convnext_multiplier_choices = [1, 2, 3]\n",
    "number_of_repeats = 10\n",
    "test_accuracies_array = np.ones((len(x), len(convnext_multiplier_choices))) \n",
    "final_population = x\n",
    "\n",
    "# Define the project path\n",
    "project_path = \"neural_evolution_final_best_evaluation_stopping_on_val_loss_10_runs_each_no_data_augmentation\"\n",
    "\n",
    "# Get all runs from the project\n",
    "project_runs = api.runs(project_path)\n",
    "\n",
    "# Dictionary to store test accuracies for each unique architecture-multiplier combination\n",
    "accuracy_dict = defaultdict(list)\n",
    "\n",
    "framework = MetaNeuralFramework(image_size, num_classes, final_epochs, batch_size, final_population, class_weights, device , use_wandb, dataset_path, data_augmentation=data_augmentation )\n",
    "\n",
    "# Iterate over the runs\n",
    "for run in project_runs:\n",
    "    # Extract the name of the run\n",
    "    run_name = run.name\n",
    "    \n",
    "    # Split the run name to extract the architecture and multiplier\n",
    "    try:\n",
    "        # Assuming the name format is consistent as [architecture]_mult_multiplier_run_nr_number\n",
    "        parts = run_name.split('_')\n",
    "        architecture = parts[0]\n",
    "        multiplier = parts[2]\n",
    "\n",
    "        architecture_to_list = ast.literal_eval(architecture)\n",
    "\n",
    "        layer_norm = framework.layer_norm[architecture_to_list[0]]\n",
    "        activation_choice = framework.activations[architecture_to_list[1]]\n",
    "        pooling_choice = framework.pooling_layer_choices[architecture_to_list[2]]\n",
    "        dropout = framework.dropout[architecture_to_list[3]]\n",
    "        learning_rate = framework.learning_rate[architecture_to_list[4]]\n",
    "        use_residual = framework.use_residual[architecture_to_list[5]]\n",
    "\n",
    "        architecture = [layer_norm, activation_choice, pooling_choice, dropout, learning_rate, use_residual]\n",
    "        \n",
    "        # Get the test accuracy from the run's summary\n",
    "        test_accuracy = run.summary.get('test/accuracy')\n",
    "        nr_of_params = run.config.get('num_of_params')\n",
    "\n",
    "        if nr_of_params is None:\n",
    "            raise ValueError(\"Number of parameters not found in the run's configuration\")\n",
    "        \n",
    "        if test_accuracy is not None:\n",
    "            # Store the test accuracy in the dictionary\n",
    "            key = f\"{architecture}_mult_{multiplier}\"\n",
    "            accuracy_dict[key].append((test_accuracy, nr_of_params))\n",
    "    except (IndexError, AttributeError) as e:\n",
    "        print(f\"Error processing run {run_name}: {e}\")\n",
    "\n",
    "# Function to calculate mean and standard deviation\n",
    "def calculate_mean_and_std(accuracies):\n",
    "    mean = np.mean([acc[0] for acc in accuracies])\n",
    "    std = np.std([acc[0] for acc in accuracies])  # Standard deviation\n",
    "    return mean, std\n",
    "\n",
    "# Data for DataFrame\n",
    "data = []\n",
    "\n",
    "# Calculate the average test accuracy, number of parameters, and standard deviation for each combination\n",
    "for key, accuracies in accuracy_dict.items():\n",
    "    if accuracies:\n",
    "        mean, std = calculate_mean_and_std(accuracies)\n",
    "        architecture, multiplier = key.split('_mult_')\n",
    "        accuracy_with_std = f\"{mean:.4f} (+/-{std:.4f})\"\n",
    "        num_params = np.mean([acc[1] for acc in accuracies])\n",
    "        data.append([architecture, multiplier, accuracy_with_std, num_params])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=['Architecture', 'Multiplier', 'Accuracy with Std', 'Number of Parameters'])\n",
    "\n",
    "# Pivot the DataFrame to get the desired matrix format\n",
    "pivot_df = df.pivot(index='Multiplier', columns='Architecture', values=['Accuracy with Std', 'Number of Parameters'])\n",
    "\n",
    "# Print the results\n",
    "print(\"Average Test Accuracy, Standard Deviation, and Number of Parameters for each Architecture-Multiplier combination:\")\n",
    "#print(pivot_df)\n",
    "\n",
    "def get_highest_mean_accuracies(accuracy_dict):\n",
    "    best_run_key = None\n",
    "    best_mean_accuracy = -np.inf\n",
    "    best_accuracies = []\n",
    "\n",
    "    for key, accuracies in accuracy_dict.items():\n",
    "\n",
    "        accuracies = [acc[0] for acc in accuracies]\n",
    "\n",
    "\n",
    "        if accuracies:\n",
    "            mean_accuracy = np.mean(accuracies)\n",
    "            if mean_accuracy > best_mean_accuracy:\n",
    "                best_mean_accuracy = mean_accuracy\n",
    "                best_run_key = key\n",
    "                best_accuracies = accuracies\n",
    "\n",
    "    if best_run_key is None:\n",
    "        raise ValueError(\"No runs found with valid accuracies\")\n",
    "\n",
    "    return best_run_key, best_mean_accuracy, best_accuracies\n",
    "\n",
    "\n",
    "# Get the best run key, mean accuracy, and accuracies\n",
    "\n",
    "best_run_key, best_mean_accuracy, best_accuracies = get_highest_mean_accuracies(accuracy_dict)\n",
    "print(f\"Best Architecture-Multiplier combination: {best_run_key}\")\n",
    "print(f\"Mean Test Accuracy: {best_mean_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Best run key:\", best_run_key)\n",
    "print(\"Highest mean test accuracy:\", best_mean_accuracy)\n",
    "print(\"Test accuracies that make up the best mean:\")\n",
    "print(best_accuracies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Gets stats for the best Baseline CNN model </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Architecture Stats:\n",
      "                                    mean  standard_deviation\n",
      "test_accuracy       0.7813993174061433  0.017996020279174324\n",
      "test_precision      0.8068541158339165  0.002118162060278354\n",
      "test_recall         0.7813993174061433  0.017996020279174324\n",
      "test_f1              0.7816511835497144  0.01729369296575994\n",
      "bacterial_accuracy  0.918348623853211  0.0042153054287737855\n",
      "viral_accuracy       0.7552238805970151  0.05058994554780559\n",
      "normal_accuracy      0.6688034188034189  0.07432703902105167\n",
      "Best result:  0.7986348122866894\n",
      "Best result index:  3\n",
      "Best result stats:  0.7986348122866894 0.8061901546653281 0.7986348122866894 0.7982339482035974 0.9174311926605504 0.7089552238805971 0.7393162393162394\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Initialize the API\n",
    "api = wandb.Api()\n",
    "\n",
    "def get_stats(wandb_project_name):\n",
    "    results = defaultdict(list) \n",
    "    project_runs = api.runs(wandb_project_name)\n",
    "\n",
    "    print(f\"Number of runs in project {wandb_project_name}: {len(project_runs)}\")\n",
    "\n",
    "    for run in project_runs:\n",
    "        test_accuracy = run.summary.get(\"test/accuracy\")\n",
    "        test_precision = run.summary.get(\"test/precision\")\n",
    "        test_recall = run.summary.get(\"test/recall\")\n",
    "        test_f1 = run.summary.get(\"test/f1\")\n",
    "        bacterial_accuracy = run.summary.get(\"test/accuracy_class_Bacterial\")\n",
    "        viral_accuracy = run.summary.get(\"test/accuracy_class_Viral\")\n",
    "        normal_accuracy = run.summary.get(\"test/accuracy_class_Normal\")\n",
    "\n",
    "        results[\"test_accuracy\"].append(test_accuracy)\n",
    "        results[\"test_precision\"].append(test_precision)\n",
    "        results[\"test_recall\"].append(test_recall)\n",
    "        results[\"test_f1\"].append(test_f1)\n",
    "        results[\"bacterial_accuracy\"].append(bacterial_accuracy)\n",
    "        results[\"viral_accuracy\"].append(viral_accuracy)\n",
    "        results[\"normal_accuracy\"].append(normal_accuracy)\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_best_architecture_stats(wandb_project_name):\n",
    "    project_runs = api.runs(wandb_project_name)\n",
    "    accuracy_dict = defaultdict(list)\n",
    "\n",
    "    for run in project_runs:\n",
    "        run_name = run.name\n",
    "        try:\n",
    "            parts = run_name.split('_')\n",
    "            architecture = parts[0]\n",
    "            multiplier = parts[2]\n",
    "\n",
    "            architecture_to_list = ast.literal_eval(architecture)\n",
    "\n",
    "            test_accuracy = run.summary.get(\"test/accuracy\")\n",
    "            if test_accuracy is not None:\n",
    "                key = f\"{architecture}_mult_{multiplier}\"\n",
    "                accuracy_dict[key].append((test_accuracy, run.summary, run.config.get('num_of_params')))\n",
    "\n",
    "        except (IndexError, AttributeError) as e:\n",
    "            print(f\"Error processing run {run_name}: {e}\")\n",
    "\n",
    "    def calculate_mean_and_std(accuracies):\n",
    "        mean = np.mean([acc[0] for acc in accuracies])\n",
    "        std = np.std([acc[0] for acc in accuracies])\n",
    "        return mean, std\n",
    "\n",
    "    best_run_key, best_mean_accuracy, _ = get_highest_mean_accuracies(accuracy_dict)\n",
    "\n",
    "    best_architecture_stats = defaultdict(list)\n",
    "    for accuracy, summary, _ in accuracy_dict[best_run_key]:\n",
    "        best_architecture_stats[\"test_accuracy\"].append(summary.get(\"test/accuracy\"))\n",
    "        best_architecture_stats[\"test_precision\"].append(summary.get(\"test/precision\"))\n",
    "        best_architecture_stats[\"test_recall\"].append(summary.get(\"test/recall\"))\n",
    "        best_architecture_stats[\"test_f1\"].append(summary.get(\"test/f1\"))\n",
    "        best_architecture_stats[\"bacterial_accuracy\"].append(summary.get(\"test/accuracy_class_Bacterial\"))\n",
    "        best_architecture_stats[\"viral_accuracy\"].append(summary.get(\"test/accuracy_class_Viral\"))\n",
    "        best_architecture_stats[\"normal_accuracy\"].append(summary.get(\"test/accuracy_class_Normal\"))\n",
    "\n",
    "    return best_architecture_stats\n",
    "\n",
    "def make_table_with_mean_and_sd(results_dict):\n",
    "    results_df = pd.DataFrame(results_dict)\n",
    "    \n",
    "    # Calculate the mean and standard deviation\n",
    "    mean_values = results_df.mean()\n",
    "    std_values = results_df.std()\n",
    "    \n",
    "    # Combine the results into one DataFrame with the format \"mean  standard deviation\"\n",
    "    formatted_results = mean_values.astype(str) + \"  \" + std_values.astype(str)\n",
    "    formatted_results_df = pd.DataFrame(formatted_results, columns=[\"mean  standard_deviation\"])\n",
    "    \n",
    "    return formatted_results_df\n",
    "\n",
    "# Get stats for the best architecture\n",
    "best_architecture_stats = get_best_architecture_stats(project_path)\n",
    "best_architecture_table = make_table_with_mean_and_sd(best_architecture_stats)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best Architecture Stats:\")\n",
    "print(best_architecture_table)\n",
    "\n",
    "# get the stats of the best run \n",
    "\n",
    "best_result = max(best_architecture_stats[\"test_accuracy\"])\n",
    "\n",
    "best_result_index = best_architecture_stats[\"test_accuracy\"].index(best_result)\n",
    "print(\"Best result: \", best_result)\n",
    "print(\"Best result index: \", best_result_index)\n",
    "print(\"Best result stats: \", best_architecture_stats[\"test_accuracy\"][best_result_index], best_architecture_stats[\"test_precision\"][best_result_index], best_architecture_stats[\"test_recall\"][best_result_index], best_architecture_stats[\"test_f1\"][best_result_index], best_architecture_stats[\"bacterial_accuracy\"][best_result_index], best_architecture_stats[\"viral_accuracy\"][best_result_index], best_architecture_stats[\"normal_accuracy\"][best_result_index])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
